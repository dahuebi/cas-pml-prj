{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data aquisition\n",
    "Download data from https://coinmarketcap.com/ and store it into a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed modules\n",
    "# standard modules\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import datetime\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "import io\n",
    "import concurrent.futures\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "# pypy modules\n",
    "import requests\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main url of coinmarketcap\n",
    "COINMARKETCAP_URL = \"https://coinmarketcap.com\"\n",
    "# url to download the currencies (coins/tokens)\n",
    "CURRENCY_URL = COINMARKETCAP_URL + \"/{}/views/all\"\n",
    "# url to get historical data per coin\n",
    "SLUG_URL = COINMARKETCAP_URL + \"/currencies/{}/historical-data/?start={}&end={}\"\n",
    "\n",
    "# directory of this projects root, jupyter must be started accordingly\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# directory for the cache\n",
    "CACHE_DIR = os.path.join(ROOT_DIR, \"cache\")\n",
    "# resulting csv file holding **all** data\n",
    "DATA_CSV = os.path.join(ROOT_DIR, \"coinmarketcap.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions from third-party modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the coin/token list returned as HTML code\n",
    "Source: https://github.com/prouast/coinmarketcap-scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCoinTokenList(html, type):\n",
    "    \"\"\"Parse the information returned by requestList for view 'all'.\"\"\"\n",
    "    data = []\n",
    "    docRoot = lxml.html.fromstring(html)\n",
    "    rows = docRoot.cssselect(\n",
    "        \"table#{0}-all > tbody > tr\".format(type))\n",
    "\n",
    "    for row in rows:\n",
    "        datum = {}\n",
    "        fields = row.cssselect(\"td\")\n",
    "\n",
    "        # Name and slug\n",
    "        nameField = fields[1].cssselect(\"a\")[0]\n",
    "        datum['name'] = nameField.text_content().strip()\n",
    "        datum['slug'] = nameField.attrib['href'].replace(\n",
    "            '/currencies/', '').replace('/', '').strip()\n",
    "\n",
    "        # Symbol\n",
    "        datum['symbol'] = fields[2].text_content().strip()\n",
    "\n",
    "        # Explorer link\n",
    "        supplyFieldPossible = fields[5].cssselect(\"a\")\n",
    "        if len(supplyFieldPossible) > 0:\n",
    "            datum['explorer_link'] = supplyFieldPossible[0].attrib['href']\n",
    "        else:\n",
    "            datum['explorer_link'] = ''\n",
    "        data.append(datum)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the historical data\n",
    "Source: https://github.com/jhogan4288/coinmarketcap-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseHistoricalData(html):\n",
    "    \"\"\"\n",
    "    Extract the price history from the HTML.\n",
    "    \n",
    "    The CoinMarketCap historical data page has just one HTML table. \n",
    "    This table contains the data we want.\n",
    "    It's got one header row with the column names.\n",
    "    \n",
    "    We need to derive the \"average\" price for the provided data.\n",
    "    \"\"\"\n",
    "    \n",
    "    head = re.search(r'<thead>(.*)</thead>', html, re.DOTALL).group(1)\n",
    "    header = re.findall(r'<th .*>([\\w ]+)</th>', head)\n",
    "    \n",
    "    body = re.search(r'<tbody>(.*)</tbody>', html, re.DOTALL).group(1)\n",
    "    raw_rows = re.findall(r'<tr[^>]*>' +\n",
    "                          r'\\s*<td[^>]*>([^<]+)</td>'*7 +\n",
    "                          r'\\s*</tr>', body)\n",
    "    \n",
    "    # strip commas\n",
    "    rows = []\n",
    "    for row in raw_rows:\n",
    "        row = [ re.sub(\",\", \"\", field) for field in row ]\n",
    "        row = [ re.sub(\"-\", \"0\", field) for field in row ]\n",
    "        # convert date\n",
    "        row[0]= datetime.datetime.strptime(row[0], \"%b %d %Y\").strftime(\"%Y%m%d\")\n",
    "        rows.append(row)\n",
    "    \n",
    "    return header, rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert between datetime object and string representation \"YYYYMMDD\"\n",
    "string2datetime = lambda s: datetime.datetime.strptime(s, \"%Y%m%d\")\n",
    "datetime2string = lambda dt: dt.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory if it does not exist\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a cache is introduced. Data downloaded from *coinmarketcap.com* are stored in this cache.\n",
    "With the cache it is not needed to download every time all historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cached data\n",
    "def loadCache(path):\n",
    "    path = os.path.abspath(path)\n",
    "    try:\n",
    "        with codecs.open(path, \"r\", encoding=\"UTF8\") as fp:\n",
    "            return fp.read()\n",
    "    except OSError:\n",
    "        pass\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cached data\n",
    "def saveCache(path, content):\n",
    "    path = os.path.abspath(path)\n",
    "    mkdir(os.path.dirname(path))\n",
    "    with codecs.open(path, \"w\", encoding=\"UTF8\") as fp:\n",
    "        fp.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a `main` method for asyncio. This function downloads the *urls* parallel and stores the *responses* for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(urls, responses):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    \n",
    "        loop = asyncio.get_event_loop()\n",
    "        futures = [\n",
    "            loop.run_in_executor(\n",
    "                None, \n",
    "                requests.get, \n",
    "                url,\n",
    "            )\n",
    "            for url in urls\n",
    "        ]\n",
    "        for response in await asyncio.gather(*futures):\n",
    "            responses.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download function for coins/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the cache, encode currency data with json\n",
    "def decodeJson(rawData):\n",
    "    try:\n",
    "        return json.loads(rawData)\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        pass\n",
    "    return []\n",
    "def encodeJson(pythonDict):\n",
    "    return json.dumps(pythonDict, indent=4)\n",
    "\n",
    "# download coins and tokens from the cache\n",
    "def getCoinsAndTokens(forceUpdate=False):\n",
    "    # forceUpdate: do not use the cache\n",
    "    # cache path for coins\n",
    "    cacheCoins = os.path.join(CACHE_DIR, \"coins.json\")\n",
    "    # cache path for tokens\n",
    "    cacheTokens = os.path.join(CACHE_DIR, \"tokens.json\")\n",
    "    coins, tokens = [], []\n",
    "    if not forceUpdate:\n",
    "        # load coins and tokens from the cache\n",
    "        coins = decodeJson(loadCache(cacheCoins))\n",
    "        tokens = decodeJson(loadCache(cacheTokens))\n",
    "\n",
    "    # early return, coins/tokens loaded from the cache\n",
    "    if coins and tokens:\n",
    "        print(\"Cached: Coins: {}, Tokens: {}\".format(len(coins), len(tokens)))\n",
    "        return coins, tokens\n",
    "\n",
    "    # load coins/tokens from the web\n",
    "    # initalize asyncio\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    # get urls to be downloaded\n",
    "    urls = [CURRENCY_URL.format(type) for type in [\"coins\", \"tokens\"]]\n",
    "    responses = []\n",
    "    # download urls in parallel\n",
    "    loop.run_until_complete(main(urls, responses))\n",
    "\n",
    "    # parse the responses\n",
    "    coins = parseCoinTokenList(responses[0].content, \"currencies\")\n",
    "    tokens = parseCoinTokenList(responses[1].content, \"assets\")\n",
    "\n",
    "    # update cache\n",
    "    saveCache(cacheCoins, encodeJson(coins))\n",
    "    saveCache(cacheTokens, encodeJson(tokens))\n",
    "    print(\"Coins: {}, Tokens: {}\".format(len(coins), len(tokens)))\n",
    "    return coins, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download function for historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct/generate the currency url based on the slug\n",
    "# start/end may be provided, otherwise, the whole history is downloaded\n",
    "def genCurrencySlugUrl(slug, start=None, end=None):\n",
    "    start = start or string2datetime(\"20100101\")\n",
    "    end = end or datetime.datetime.utcnow() + datetime.timedelta(days=1)\n",
    "    return SLUG_URL.format(slug, datetime2string(start), datetime2string(end))\n",
    "\n",
    "# get the cache path for a given slug\n",
    "def getSlugCache(slug):\n",
    "    return os.path.join(CACHE_DIR, \"{}.csv\".format(slug))\n",
    "\n",
    "# encode historical data with csv\n",
    "def encodeCsv(data):\n",
    "    fp = io.StringIO()\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerows(data)\n",
    "    return fp.getvalue()\n",
    "\n",
    "def decodeCsv(raw):\n",
    "    reader = csv.reader(raw.splitlines())\n",
    "    return list(reader)\n",
    "\n",
    "# only keep the date part of the datetime object\n",
    "striptime = lambda dt: datetime.datetime.combine(dt.date(), datetime.time())\n",
    "\n",
    "# parse response for a slug and save the data to the cache\n",
    "def parseResponseSaveCache(slug, response):\n",
    "    # parse historical data\n",
    "    _, rawData = parseHistoricalData(response.content.decode(\"UTF8\"))\n",
    "    # get the cache file\n",
    "    path = getSlugCache(slug)\n",
    "    # load the cache\n",
    "    rows = decodeCsv(loadCache(path))\n",
    "    # append new date\n",
    "    rows.extend(rawData)\n",
    "    # sort by date\n",
    "    rows = sorted(rows, key=lambda r: int(r[0]))\n",
    "    # update the cache\n",
    "    saveCache(path, encodeCsv(rows))\n",
    "\n",
    "# download **all** historical data of **all** slugs\n",
    "# use a cache to make it faster on successive runs\n",
    "# the function returns the number of updated histories\n",
    "def getHistories(slugs):\n",
    "    # build requests\n",
    "    requests = []\n",
    "    # keep track which request belongs to which slug\n",
    "    slugRequestMap = {}\n",
    "    # current utc time, historical data are update on UTC 00:00:00\n",
    "    utcnow = striptime(datetime.datetime.utcnow())\n",
    "\n",
    "    # for all slugs, prepare the url\n",
    "    for slug in slugs:\n",
    "        path = getSlugCache(slug)\n",
    "        dtCache = None\n",
    "        if os.path.exists(path):\n",
    "            # get the timestamp of the cached file of the slug\n",
    "            st = os.stat(path)\n",
    "            dtCache = datetime.datetime.utcfromtimestamp(st.st_mtime)\n",
    "            dtCache = striptime(dtCache)\n",
    "        # load the cached file\n",
    "        rows = decodeCsv(loadCache(path))\n",
    "        \n",
    "        # find the date of the next entry\n",
    "        start = None\n",
    "        if rows:\n",
    "            # get latest date\n",
    "            start = string2datetime(rows[-1][0])\n",
    "            # add one day\n",
    "            start += datetime.timedelta(days=1)\n",
    "\n",
    "        if start:\n",
    "            # if start lies in the future, skip\n",
    "            if start >= utcnow:\n",
    "                continue\n",
    "            # if the cache date current, skip\n",
    "            if dtCache and dtCache >= utcnow:\n",
    "                continue\n",
    "\n",
    "        # build the url for the slug\n",
    "        url = genCurrencySlugUrl(slug, start)\n",
    "        # append to requests\n",
    "        requests.append(url)\n",
    "        # add to inverse mapping\n",
    "        slugRequestMap[url] = slug\n",
    "\n",
    "    # nothing to download, return\n",
    "    if not slugRequestMap:\n",
    "        return 0\n",
    "\n",
    "    # prepare asyncio\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    responses = []\n",
    "    while requests:\n",
    "        print(\"\\rRequest to process: {}{}\".format(len(requests), \" \"*20), flush=True, end=\"\")\n",
    "        # download all requests\n",
    "        loop.run_until_complete(main(requests, responses))\n",
    "       \n",
    "        # check responses, try again if it failed\n",
    "        requests = []\n",
    "        for r in responses:\n",
    "            # remove responses\n",
    "            responses.remove(r)\n",
    "            if r.ok:\n",
    "                parseResponseSaveCache(slugRequestMap[r.url], r)\n",
    "            else:\n",
    "                # print(\"Failed: {}\".format(url))\n",
    "                pass\n",
    "        responses = []\n",
    "    print(\"\")  # add newline feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to build the final CSV file holding all currency data\n",
    "This function reads all cached coin/token data and merges it into a single *csv* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all cached csv into a single csv\n",
    "def buildAllCurrenciesCsv(allCurrencies):\n",
    "    # count rows\n",
    "    rowCnt = 0\n",
    "    with codecs.open(DATA_CSV, \"w\", encoding=\"UTF8\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow([\n",
    "                \"date\",\n",
    "                \"slug\",\n",
    "                \"name\",\n",
    "                \"open\",\n",
    "                \"high\",\n",
    "                \"low\",\n",
    "                \"close\",\n",
    "                \"volume\",\n",
    "                \"marketcap\"])\n",
    "        # for each currency append to the data file\n",
    "        # and insert *slug* and *name* as column\n",
    "        for currency in allCurrencies:\n",
    "            slug = currency[\"slug\"]\n",
    "            name = currency[\"name\"]\n",
    "            path = getSlugCache(slug)\n",
    "            rows = decodeCsv(loadCache(path))\n",
    "            print(\"\\r{}/{}{}\".format(slug, len(rows), \" \"*20), end=\"\", flush=True)\n",
    "            for row in rows:\n",
    "                writer.writerow([row[0]] + [slug, name] + row[1:])\n",
    "                rowCnt += 1\n",
    "    print(\"\\rCurrencies: {}, rows: {}\".format(len(allCurrencies), rowCnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CACHE: /home/dahuebi/PML/cas-pml-prj/cache\n",
      "DATA:  /home/dahuebi/PML/cas-pml-prj/coinmarketcap.csv\n",
      "Coins: 917, Tokens: 677\n",
      "Request to process: 1594                    \n",
      "Currencies: 1594, rows: 750954                         \n"
     ]
    }
   ],
   "source": [
    "print(\"CACHE: {}\".format(CACHE_DIR))\n",
    "print(\"DATA:  {}\".format(DATA_CSV))\n",
    "# download coins and tokens\n",
    "coins, tokens = getCoinsAndTokens(forceUpdate=True)\n",
    "allCurrencies = coins + tokens\n",
    "\n",
    "# get the slug name from the dicts\n",
    "slugs = [x[\"slug\"] for x in allCurrencies]\n",
    "# download historical data\n",
    "getHistories(slugs)\n",
    "\n",
    "# always build CSV\n",
    "buildAllCurrenciesCsv(allCurrencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
